{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>1. Scrapping Process </h1>\n",
      "For this project we scrapped the popular website eBay for analyzing the bidding mechanism of eBay online bidding. More specifically, we looked at the eBay motor part of the website for getting bidding information of the motor vehicles auctioned on eBay. We designed a two step scrapping process for the purpose of geting all the information:\n",
      "\n",
      "* eBay only shows 10k items in the search. So to maximize out dataset we scrapped each model separately. In this step we scrapped all the search result pages for each model and scraped item id, car mileage, car year, and model data. \n",
      "\n",
      "* Then we tried to scrape the bid history of each of the items by using their item ids. However, soon we found that eBay's security system blocks our ip and asks for solving captcha after we scrape a few hundred pages (most likely due to maximum call limit per day). To get around that we designed a dostributed scraping system using tor and polipo client. Tor clinet broadcasts at port 9050 and we made our polipo client listen to that port. Our scrapped connects with port 8123 (polipo port) and channel the data trhough the tor. Therefore we get new ip every once in a while. To force the the ip renewal process we inclused a network restart bash script and that forces ip switching each time we get error (presumably when eBay asks for solving captcha).\n",
      "\n",
      "\n",
      "<h2> Scraping Item ID: </h2>\n",
      "Using the script below we scrapped all the item ids, car milage, car yea, and car model for about 51k cars. To make this process fast we used grequests library that enables us to use asynchronous connnections. This way using a pool of connections we scraped data very fast."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# library imports\n",
      "\n",
      "import pandas as pd\n",
      "from lxml import html, cssselect\n",
      "import requests\n",
      "import grequests\n",
      "from lxml.cssselect import CSSSelector\n",
      "import numpy as np\n",
      "import cPickle as pickle\n",
      "\n",
      "\n",
      "import random\n",
      "import numpy as np\n",
      "import os, sys\n",
      "\n",
      "random.seed(42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# functions for scraping the search result pages\n",
      "\n",
      "# the following get functions use cssselect to get and process parts of html content\n",
      "def get_center_content(content):\n",
      "\treturn content.cssselect('div#ResultSetItems')[0]\n",
      "def get_itm_tables(content):\n",
      "\treturn [s for s in content.cssselect('table.li.rsittlref')]\n",
      "def get_items(content):\n",
      "\treturn [s.text_content().strip() for s in content.cssselect('div.dynS')]\n",
      "def get_itm_table_cont(content):\n",
      "\treturn [s.text_content().strip() for s in content.cssselect('table.li.rsittlref')]\n",
      "def get_itm_name(content):\n",
      "\treturn content.cssselect('div.ittl')[0].text_content().strip()\n",
      "def get_milage_year(content):\n",
      "\treturn [s.text_content().strip() for s in content.cssselect('span.v')]\n",
      "\n",
      "# gets all the models (scrapped from eBay)\n",
      "def get_models():\n",
      "\tbrandArr = []\n",
      "\tstrtest  = '{ name : \"AC\"},{ name : \"AM General\"},{ name : \"Abarth\"},{ name : \"Acura\"},{ name : \"Alfa Romeo\", alt : \"Alfa%20Romeo\"},{ name : \"Allard\"},{ name : \"Allstate\"},{ name : \"Alpine\"},{ name : \"Alvis\"},{ name : \"American Austin\"},{ name : \"American Bantam\"},{ name : \"American Motors\"},{ name : \"Amphicar\"},{ name : \"Apollo\"},{ name : \"Apperson\"},{ name : \"Armstrong-Siddeley\"},{ name : \"Arnolt-Bristol\"},{ name : \"Arnolt-MG\"},{ name : \"Aston Martin\", alt : \"Aston%20Martin\"},{ name : \"Asuna\"},{ name : \"Auburn\"},{ name : \"Audi\"},{ name : \"Austin\"},{ name : \"Austin Healey\", alt : \"Austin%20Healey\"},{ name : \"Avanti\"},{ name : \"BMW\"},{ name : \"Bentley\"},{ name : \"Berkeley\"},{ name : \"Bertone\"},{ name : \"Biddle\"},{ name : \"Bizzarrini\"},{ name : \"Blackhawk\"},{ name : \"Bond\"},{ name : \"Borgward\"},{ name : \"Bricklin\"},{ name : \"Bristol\"},{ name : \"Bugatti\"},{ name : \"Buick\"},{ name : \"Cadillac\"},{ name : \"Case\"},{ name : \"Chandler\"},{ name : \"Checker\"},{ name : \"Chevrolet\"},{ name : \"Chrysler\"},{ name : \"Cisitalia\"},{ name : \"Citroen\"},{ name : \"Cleveland\"},{ name : \"Coda\"},{ name : \"Cole\"},{ name : \"Continental\"},{ name : \"Cord\"},{ name : \"Crosley\"},{ name : \"Cunningham\"},{ name : \"DAF\"},{ name : \"DKW\"},{ name : \"Daewoo\"},{ name : \"Daihatsu\"},{ name : \"Daimler\"},{ name : \"Darrin\"},{ name : \"Davis\"},{ name : \"De Vaux\"},{ name : \"DeLorean\"},{ name : \"DeSoto\"},{ name : \"DeTomaso\"},{ name : \"Delage\"},{ name : \"Delahaye\"},{ name : \"Dellow\"},{ name : \"Denzel\"},{ name : \"Deutsch-Bonnet\"},{ name : \"Diana\"},{ name : \"Dodge\"},{ name : \"Doretti\"},{ name : \"Du Pont\"},{ name : \"Dual-Ghia\"},{ name : \"Duesenberg\"},{ name : \"Durant\"},{ name : \"Duryea\"},{ name : \"Eagle\"},{ name : \"Edsel\"},{ name : \"Elcar\"},{ name : \"Elva\"},{ name : \"Erskine\"},{ name : \"Essex\"},{ name : \"Excalibur\"},{ name : \"FWD\"},{ name : \"Facel Vega\"},{ name : \"Fairthorpe\"},{ name : \"Falcon Knight\"},{ name : \"Fargo\"},{ name : \"Ferrari\"},{ name : \"Fiat\"},{ name : \"Flint\"},{ name : \"Ford\"},{ name : \"Franklin\"},{ name : \"Frazer\"},{ name : \"Frazer Nash\"},{ name : \"Freightliner\"},{ name : \"GMC\"},{ name : \"Gardner\"},{ name : \"Geo\"},{ name : \"Glas\"},{ name : \"Goggomobil\"},{ name : \"Goliath\"},{ name : \"Gordon-Keeble\"},{ name : \"Graham\"},{ name : \"Graham-Paige\"},{ name : \"Griffith\"},{ name : \"HCS\"},{ name : \"HRG\"},{ name : \"Hansa\"},{ name : \"Haynes\"},{ name : \"Healey\"},{ name : \"Henry J\"},{ name : \"Hertz\"},{ name : \"Hillman\"},{ name : \"Hino\"},{ name : \"Hispano-Suiza\"},{ name : \"Honda\"},{ name : \"Hotchkiss\"},{ name : \"Hudson\"},{ name : \"Humber\"},{ name : \"Hummer\"},{ name : \"Hupmobile\"},{ name : \"Hyundai\"},{ name : \"Infiniti\"},{ name : \"International\"},{ name : \"Iso\"},{ name : \"Isotta Fraschini\"},{ name : \"Isuzu\"},{ name : \"Iveco\"},{ name : \"Jaguar\"},{ name : \"Jeep\"},{ name : \"Jeffery\"},{ name : \"Jensen\"},{ name : \"Jewett\"},{ name : \"Jordan\"},{ name : \"Jowett\"},{ name : \"Kaiser\"},{ name : \"Kenworth\"},{ name : \"Kia\"},{ name : \"Kissel\"},{ name : \"Kurtis\"},{ name : \"LaSalle\"},{ name : \"Lada\"},{ name : \"Laforza\"},{ name : \"Lagonda\"},{ name : \"Lamborghini\"},{ name : \"Lanchester\"},{ name : \"Lancia\"},{ name : \"Land Rover\", alt : \"Land%20Rover\"},{ name : \"Lea-Francis\"},{ name : \"Lexington\"},{ name : \"Lexus\"},{ name : \"Lincoln\"},{ name : \"Lloyd\"},{ name : \"Locomobile\"},{ name : \"Lotus\"},{ name : \"MG\"},{ name : \"Mack\"},{ name : \"Maico\"},{ name : \"Marathon\"},{ name : \"Marauder\"},{ name : \"Marcos\"},{ name : \"Marmon\"},{ name : \"Marquette\"},{ name : \"Maserati\"},{ name : \"Matra\"},{ name : \"Maxwell\"},{ name : \"Maybach\"},{ name : \"Mazda\"},{ name : \"McLaren\"},{ name : \"Mercedes-Benz\", alt : \"Mercedes%2DBenz\"},{ name : \"Mercury\"},{ name : \"Merkur\"},{ name : \"Messerschmitt\"},{ name : \"Metropolitan\"},{ name : \"Mini\"},{ name : \"Mitsubishi\"},{ name : \"Mitsubishi Fuso\"},{ name : \"Monteverdi\"},{ name : \"Moon\"},{ name : \"Moretti\"},{ name : \"Morgan\"},{ name : \"Morris\"},{ name : \"Moskvich\"},{ name : \"NSU\"},{ name : \"Nardi\"},{ name : \"Nash\"},{ name : \"Nissan\"},{ name : \"Oakland\"},{ name : \"Oldsmobile\"},{ name : \"Omega\"},{ name : \"Opel\"},{ name : \"Osca\"},{ name : \"Packard\"},{ name : \"Paige\"},{ name : \"Panhard\"},{ name : \"Panoz\"},{ name : \"Panther\"},{ name : \"Peerless\"},{ name : \"Pegaso\"},{ name : \"Peterbilt\"},{ name : \"Peugeot\"},{ name : \"Pierce-Arrow\"},{ name : \"Plymouth\"},{ name : \"Pontiac\"},{ name : \"Porsche\"},{ name : \"Qvale\"},{ name : \"Ram\"},{ name : \"Rambler\"},{ name : \"Reliant\"},{ name : \"Renault\"},{ name : \"Reo\"},{ name : \"Rickenbacker\"},{ name : \"Riley\"},{ name : \"Roamer\"},{ name : \"Rockne\"},{ name : \"Rollin\"},{ name : \"Rolls Royce\"},{ name : \"Roosevelt\"},{ name : \"Rover\"},{ name : \"SRT\"},{ name : \"Saab\"},{ name : \"Sabra\"},{ name : \"Saleen\"},{ name : \"Salmson\"},{ name : \"Saturn\"},{ name : \"Scion\"},{ name : \"Scripps Booth\"},{ name : \"Shelby\"},{ name : \"Sheridan\"},{ name : \"Siata\"},{ name : \"Simca\"},{ name : \"Singer\"},{ name : \"Skoda\"},{ name : \"Smart\"},{ name : \"Standard\"},{ name : \"Stanguellini\"},{ name : \"Star\"},{ name : \"Stearns Knight\"},{ name : \"Sterling\"},{ name : \"Stevens-Duryea\"},{ name : \"Studebaker\"},{ name : \"Stutz\"},{ name : \"Subaru\"},{ name : \"Sunbeam\"},{ name : \"Suzuki\"},{ name : \"Swallow\"},{ name : \"TVR\"},{ name : \"Talbot-Lago\"},{ name : \"Tatra\"},{ name : \"Tesla\"},{ name : \"Toyopet\"},{ name : \"Toyota\"},{ name : \"Triumph\"},{ name : \"Tucker\"},{ name : \"Turner\"},{ name : \"UD\"},{ name : \"VPG\"},{ name : \"Vauxhall\"},{ name : \"Velie\"},{ name : \"Vespa\"},{ name : \"Viking\"},{ name : \"Volkswagen\"},{ name : \"Volvo\"},{ name : \"Wartburg\"},{ name : \"Westcott\"},{ name : \"Whippet\"},{ name : \"Willys\"},{ name : \"Windsor\"},{ name : \"Wolseley\"},{ name : \"Workhorse\"},{ name : \"Yellow Cab\"},{ name : \"Yugo\"},{ name : \"Zundapp\"}'\n",
      "\tfor s  in strtest.replace(\"{\",\"\").replace(\"}\",\"\").split(','):\n",
      "\t\tn = s.split(':')[1].replace(\"\\\"\",\"\")\n",
      "\t\tbrandArr.append(n)\n",
      "\treturn brandArr\n",
      "\n",
      "# gets the total number of search results from the first search page of a specific model\n",
      "def get_total_result(link):\n",
      "\tlink = link.replace(\"PAGE_NO\", '1')\n",
      "\ttry:\n",
      "\t\tpage = requests.get(link)\n",
      "\t\ttree = html.fromstring(page.text)\n",
      "\t\tcenter = tree[1][2]\n",
      "\t\treturn int(center.cssselect('span.rcnt')[0].text_content().replace(\",\",\"\"))/50 + 1\n",
      "\texcept Exception, e:\n",
      "\t\tprint 'Failed to estimate exact total page numbers', e\n",
      "\t\t# or try to scrape 5 pages\n",
      "\t\treturn 5\n",
      "\n",
      "# scrapes a rearch result page for item no. and item info\n",
      "def get_info_apage(page, make, info_arr):\n",
      "\tinfo = []\n",
      "\ttree = html.fromstring(page.text)\n",
      "    \n",
      "\tcenter = tree[1][2][1][2]\n",
      "\ttable = get_center_content(center)\n",
      "    \n",
      "\titem_nos = get_itm_tables(table)\n",
      "\ttotal_elems = len(item_nos)\n",
      "    \n",
      "\tprint total_elems, make\n",
      "    \n",
      "\tfor k in range(total_elems):\n",
      "\t\td = item_nos[k]\n",
      "\t\t# item # - name - milage - year - make\n",
      "\t\tmilage_yr = get_milage_year(d)\n",
      "\t\tprod_id = int(d.get('listingid'))\n",
      "\t\tprod_name = get_itm_name(d)\n",
      "\n",
      "\t\ttry:\n",
      "\t\t\tprod_milage = int(milage_yr[1].replace(\",\",\"\"))\n",
      "\t\texcept Exception, e:\n",
      "\t\t\t# print \"no valid milage info\"\n",
      "\t\t\tprod_milage = -1\n",
      "\t\t\n",
      "\t\ttry:\n",
      "\t\t\tprod_yr = int(milage_yr[0])\n",
      "\t\texcept Exception, e:\n",
      "\t\t\t# print \"no valid year info\"\n",
      "\t\t\tprod_yr = 0\n",
      "\t\tinfo.append([prod_id, prod_name, prod_milage, prod_yr, make])\n",
      "\n",
      "\t\tinfo_arr.extend(info)\n",
      "\n",
      "\tif total_elems == 0:\n",
      "\t\treturn False\n",
      "\telse:\n",
      "\t\treturn True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Exception in thread Thread-1:\n",
        "Traceback (most recent call last):\n",
        "  File \"/home/tmoon/anaconda/lib/python2.7/threading.py\", line 808, in __bootstrap_inner\n",
        "    self.run()\n",
        "  File \"/home/tmoon/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/parentpoller.py\", line 31, in run\n",
        "    time.sleep(1.0)\n",
        "  File \"/home/tmoon/anaconda/lib/python2.7/site-packages/gevent/hub.py\", line 79, in sleep\n",
        "    switch_result = get_hub().switch()\n",
        "  File \"/home/tmoon/anaconda/lib/python2.7/site-packages/gevent/hub.py\", line 135, in get_hub\n",
        "    raise NotImplementedError('gevent is only usable from a single thread')\n",
        "NotImplementedError: gevent is only usable from a single thread\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# finally scrape all the pages and pickle the dataframe\n",
      "\n",
      "# NOTE: we did all the scrapping with a separate .py file. So no result is showed here\n",
      "prod_info_arr = []\n",
      "models = get_models()\n",
      "\n",
      "print 'number of models', len(models)\n",
      "\n",
      "for model in models:\n",
      "    link_struct = 'http://www.ebay.com/sch/Cars-Trucks-/6001/i.html?LH_Auction=1&LH_Complete=1&LH_Sold=1&makeval=BRAND_NAME&_nkw=BRAND_NAME&_pgn=PAGE_NO&rt=nc'\n",
      "    link_struct = link_struct.replace(\"BRAND_NAME\", model)\n",
      "    \n",
      "    # ebay search result is capped at 10k elements. So 199 is the max range we would like to go\n",
      "    NUM_PAGES = min(199, get_total_result(link_struct))\n",
      "    print NUM_PAGES\n",
      "    url_arr = []\n",
      "    for i in range(1,NUM_PAGES):\n",
      "        link = link_struct.replace(\"PAGE_NO\", str(i))\n",
      "        url_arr.append(link)\n",
      "\n",
      "    # async http fetching for faster response, 32 requests at the same time\n",
      "    async_processes = [grequests.get(u) for u in url_arr]\n",
      "\n",
      "    responses = grequests.map(async_processes, size = 32)\n",
      "\n",
      "    for page in responses:\n",
      "        try:\t\t\t\t\n",
      "            flag = get_info_apage(page, model, prod_info_arr)\n",
      "            if flag == False:\n",
      "                break\n",
      "        except Exception, e:\n",
      "            print e\n",
      "\n",
      "info_arr = np.array(prod_info_arr)\n",
      "df = pd.DataFrame({'prod_id':info_arr[:,0], 'prod_title':info_arr[:,1], 'prod_milage':info_arr[:,2], 'prod_yr':info_arr[:,3], 'model': info_arr[:,4]})\n",
      "df.to_pickle('data.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second process was much more involded (as explained in the previous section) due to the security of the eBay website that discourage web scraping. So in the following script using our distributed scraping technique we scrape all the time series of all the products. To get around with the security system we also randomly change the http request header. So that each time eBay server thinks that we are connecting from different brwosers. We saved the data sequentially with each file having 250 item informations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions for scrapping the time series page of bid history\n",
      "\n",
      "def get_random_agent():\n",
      "\t\"\"\"returns one of the random browser metadata for bypassing \n",
      "\t\tebay's anti scrapping AI\n",
      "\t\"\"\"\n",
      "\tUSER_AGENT_LIST = [\n",
      "\t    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 \\\n",
      "\t         (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7',\n",
      "\t    'Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0) \\\n",
      "\t       Gecko/16.0 Firefox/16.0',\n",
      "\t    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/534.55.3 \\\n",
      "\t       (KHTML, like Gecko) Version/5.1.3 Safari/534.53.10',\n",
      "\t    \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0b4pre) Gecko/20100815 Minefield/4.0b4pre\",\n",
      "\t    \"Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14\",\n",
      "\t    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.2; WOW64; Trident/5.0)\",\n",
      "\t    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 \\\n",
      "\t    \t(KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7\",\n",
      "\t    \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0b4pre) Gecko/20100815 Minefield/4.0b4pre\"\n",
      "\n",
      "\t]\n",
      "\treturn random.choice(USER_AGENT_LIST)\n",
      "\n",
      "# the following functions use cssselect to get and process parts of html content\n",
      "def get_url(pid):\n",
      "\t\"\"\"get url from product id\"\"\"\n",
      "\treturn 'http://offer.ebay.com/ws/eBayISAPI.dll?ViewBids&item='+str(pid)+'&showauto=true'\n",
      "\n",
      "def get_req_instance(u):\n",
      "\t\"\"\"create a proxy enabled and random brwoser mimicking request instance\"\"\"\n",
      "\tHTTP_PROXY ={\"http\":'http://127.0.0.1:8123'}\n",
      "\treturn requests.get(u, headers = {'user-agent': get_random_agent}, proxies=HTTP_PROXY)\n",
      "\n",
      "def getNameBid(content):\n",
      "\titemText = content.cssselect('a.BHitemDesc')[0].text_content()\n",
      "\twinningBid = content.cssselect('td.BHctBidVal')[0].text_content()\n",
      "\n",
      "\treturn itemText.replace(\"Item Title: \", \"\"), winningBid\n",
      "\n",
      "def headVals(content):\n",
      "\treturn [s.text_content().strip() for s in content.cssselect('span.titleValueFont')]\n",
      "\n",
      "def get_rows(content, pid):\n",
      "\trows = content.cssselect('tr')[1:]\n",
      "\tinfo_arr = []\n",
      "\tfor r in rows:\n",
      "\t\tauto = 1\n",
      "\t\n",
      "\t\tif r.get('class') == None:\n",
      "\t\t\tauto = 0\n",
      "\t\tif r.get('id') == 'viznobrd':\n",
      "\t\t\tauto = -1\n",
      "\n",
      "\t\tif auto == 1:\n",
      "\t\t\tvals = [s.text_content().strip() for s in r.cssselect('td.newcontentValueFont')]\n",
      "\t\telif auto == 0:\n",
      "\t\t\tvals = [r.cssselect('td.onheadNav')[0].text_content().strip()]\n",
      "\t\t\tvals.extend([s.text_content().strip() for s in r.cssselect('td.contentValueFont')])\n",
      "\t\t\tif vals[0][:10] == 'Member Id:':\n",
      "\t\t\t\tvals[0] = vals[0][11:]\n",
      "\t\telse:\n",
      "\t\t\tvals = ['Starting Price']\n",
      "\t\t\tvals.extend([s.text_content().strip() for s in r.cssselect('td.contentValueFont')])\n",
      "\n",
      "\t\tassert (len(vals) == 3)\n",
      "\t\tvals.append(auto)\n",
      "\t\tvals.append(int(pid))\n",
      "\t\tinfo_arr.append(vals)\n",
      "\treturn info_arr\n",
      "\n",
      "\n",
      "def get_bid_table(content):\n",
      "\treturn content.cssselect('div#vizrefdiv')[0]\n",
      "\n",
      "def get_table(pid):\n",
      "\turl = get_url(pid)\n",
      "\tpage = get_req_instance(url)\n",
      "\tprint page.url\n",
      "\ttree = html.fromstring(page.text)\n",
      "\tt0 = time.time()\n",
      "\tsel = CSSSelector('div.BHbidSecBorderGrey')\n",
      "\tres = sel(tree)\n",
      "\n",
      "\titm_info = [pid]\n",
      "\tname_price = getNameBid(res[0])\n",
      "\titm_info.extend(name_price)\n",
      "\t\n",
      "\tbid_data = headVals(res[1])\n",
      "\titm_info.extend(bid_data)\n",
      "\n",
      "\ttab = get_bid_table(res[1])\n",
      "\treturn itm_info, get_rows(tab, pid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## NOTE: need a folder ./data/ to save the data files\n",
      "\n",
      "basic_info = []\n",
      "time_series = []\n",
      "t0 = time.time()\n",
      "t = 0\n",
      "ferrror = open('error_log.txt','w')\n",
      "err = 0\n",
      "for k, pid in enumerate(items):\n",
      "\n",
      "    try:\n",
      "        itm_info, arr = get_table(pid)\n",
      "        time_series.extend(arr)\n",
      "        basic_info.append(itm_info)\n",
      "    except Exception, e:\n",
      "        print 'ERROR!', e\n",
      "        ferrror.write(str(pid)+'\\n')\n",
      "\n",
      "        # restart the network to update ip if necessary\n",
      "        command = 'echo '+str(sys.argv[1])+' | sudo -S service network-manager restart'\n",
      "        os.system(command)\n",
      "        err += 1\n",
      "\n",
      "    old_t = t\n",
      "    t = time.time() - t0\n",
      "    t1 = t - old_t\n",
      "\n",
      "    r = max(random.random() * 1.5 , 0.3)\n",
      "    time.sleep(r)\n",
      "    print t1 + r\n",
      "    print 'scrapped %d items (failed %d), current item id %s, time elapsed %f' % (k, err, pid, t)\n",
      "    print '\\n ------------------------------- \\n'\n",
      "\n",
      "    if k % 250 == 0 and k > 0:\n",
      "        basic_info = np.array(basic_info)\n",
      "        time_series = np.array(time_series)\n",
      "\n",
      "        df_basic = pd.DataFrame({'prod_id': basic_info[:,0], 'prod_title': basic_info[:,1], 'price': basic_info[:,2], 'bidders': basic_info[:,3], \\\n",
      "            'bids': basic_info[:,4], 'end_time': basic_info[:,5], 'duration': basic_info[:,6]})\n",
      "        print df_basic.head()\n",
      "        df_ts = pd.DataFrame({'prod_id': time_series[:,4],'bidder': time_series[:,0],'bid_amount': time_series[:,1],\\\n",
      "            'bid_time': time_series[:,2], 'auto_flag': time_series[:,3]})\n",
      "        print df_ts.head()\n",
      "        # reser buffers\n",
      "        basic_info = []\n",
      "        time_series = []\n",
      "        # save everything\n",
      "        try:\n",
      "            df_basic.to_csv('./data/basic_'+str(k/250)+'.csv', sep='\\t', index=False, header = False)\n",
      "        except Exception, e:\n",
      "            print e\n",
      "\n",
      "        try:\n",
      "            df_ts.to_csv('./data/ts_'+str(k/250)+'.csv', sep='\\t', index=False, header = False)\n",
      "        except Exception, e:\n",
      "            print e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}